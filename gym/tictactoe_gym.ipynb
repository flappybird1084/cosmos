{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgYOd28acSWZ",
        "outputId": "4fb319ab-6e81-4701-e42e-26cd1c67af74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 5000: W/D/L = 4095/356/549  (0.82/0.07/0.11)\n",
            "Episode 10000: W/D/L = 4351/293/356  (0.87/0.06/0.07)\n",
            "Episode 15000: W/D/L = 4404/309/287  (0.88/0.06/0.06)\n",
            "Episode 20000: W/D/L = 4517/242/241  (0.90/0.05/0.05)\n",
            "Episode 25000: W/D/L = 4632/193/175  (0.93/0.04/0.04)\n",
            "Episode 30000: W/D/L = 4687/173/140  (0.94/0.03/0.03)\n",
            "Episode 35000: W/D/L = 4701/177/122  (0.94/0.04/0.02)\n",
            "Episode 40000: W/D/L = 4745/162/93  (0.95/0.03/0.02)\n",
            "Episode 45000: W/D/L = 4771/138/91  (0.95/0.03/0.02)\n",
            "Episode 50000: W/D/L = 4779/142/79  (0.96/0.03/0.02)\n",
            "\n",
            "After training: W/D/L = 4963/37/0 (0.99/0.01/0.00)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# ─── 1) TicTacToe Gym Environment ───────────────────────────────────────────────\n",
        "class TicTacToeEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 0=empty, 1=X (agent), 2=O (opponent)\n",
        "        self.observation_space = spaces.Discrete(3**9)\n",
        "        self.action_space      = spaces.Discrete(9)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # create empty board\n",
        "        self.board = np.zeros(9, dtype=int)\n",
        "        self.done  = False\n",
        "        return self._encode()\n",
        "\n",
        "    def _encode(self):\n",
        "        \"\"\"Convert board[0..8] in {0,1,2} to single int in [0,3^9).\"\"\"\n",
        "        code = 0\n",
        "        for i, v in enumerate(self.board):\n",
        "            code += (3**i) * int(v)\n",
        "        return code\n",
        "\n",
        "    def _decode(self, code):\n",
        "        \"\"\"Convert int code back to board array.\"\"\"\n",
        "        b = np.zeros(9, dtype=int)\n",
        "        for i in range(9):\n",
        "            b[i] = code % 3\n",
        "            code //= 3\n",
        "        return b\n",
        "\n",
        "    def step(self, action):\n",
        "        # invalid move\n",
        "        if self.done or self.board[action] != 0:\n",
        "            return self._encode(), -10.0, True, {}\n",
        "\n",
        "        # agent move\n",
        "        self.board[action] = 1\n",
        "        if self._is_winner(1):\n",
        "            self.done = True\n",
        "            return self._encode(), +1.0, True, {}\n",
        "        if 0 not in self.board:\n",
        "            self.done = True\n",
        "            return self._encode(), 0.0, True, {}  # draw\n",
        "\n",
        "        # opponent random move\n",
        "        empties = np.where(self.board == 0)[0]\n",
        "        opp_act = random.choice(empties)\n",
        "        self.board[opp_act] = 2\n",
        "        if self._is_winner(2):\n",
        "            self.done = True\n",
        "            return self._encode(), -1.0, True, {}\n",
        "        if 0 not in self.board:\n",
        "            self.done = True\n",
        "            return self._encode(), 0.0, True, {}\n",
        "\n",
        "        # game continues\n",
        "        return self._encode(), 0.0, False, {}\n",
        "\n",
        "    def _is_winner(self, p):\n",
        "        B = self.board.reshape(3,3)\n",
        "        # rows, cols, diags\n",
        "        win_states = [B[i,:] for i in range(3)] + \\\n",
        "                     [B[:,j] for j in range(3)] + \\\n",
        "                     [B.diagonal(), np.fliplr(B).diagonal()]\n",
        "        return any((line == p).all() for line in win_states)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        chars = ['.', 'X', 'O']\n",
        "        B = [chars[v] for v in self.board]\n",
        "        for i in range(3):\n",
        "            print(' '.join(B[3*i:3*i+3]))\n",
        "        print()\n",
        "\n",
        "# ─── 2) Q‐Learning Agent ─────────────────────────────────────────────────────────\n",
        "def encode_state(board):\n",
        "    # already encoded by env, so we can reuse\n",
        "    return board\n",
        "\n",
        "def choose_action(Q, state, eps):\n",
        "    if random.random() < eps:\n",
        "        # random among legal moves\n",
        "        b = env._decode(state)\n",
        "        legal = [i for i,v in enumerate(b) if v==0]\n",
        "        return random.choice(legal)\n",
        "    # otherwise greedy among legal\n",
        "    qvals = Q[state]\n",
        "    b = env._decode(state)\n",
        "    legal = [i for i,v in enumerate(b) if v==0]\n",
        "    # pick legal action with highest Q\n",
        "    best = max(legal, key=lambda a: qvals[a])\n",
        "    return best\n",
        "\n",
        "# ─── 3) Training Loop ────────────────────────────────────────────────────────────\n",
        "env = TicTacToeEnv()\n",
        "\n",
        "# hyperparameters\n",
        "alpha   = 0.1\n",
        "gamma   = 0.9\n",
        "epsilon = 0.2\n",
        "episodes = 50_000\n",
        "\n",
        "# Q-table: default 0 for unseen states\n",
        "Q = np.zeros((3**9, 9))\n",
        "\n",
        "# stats\n",
        "wins = draws = losses = 0\n",
        "\n",
        "for ep in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done  = False\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(Q, state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # compute max_a' Q(next_state,a')\n",
        "        # but restrict to legal actions\n",
        "        b_next = env._decode(next_state)\n",
        "        legal_next = [i for i,v in enumerate(b_next) if v==0]\n",
        "        if legal_next:\n",
        "            max_q_next = max(Q[next_state, a] for a in legal_next)\n",
        "        else:\n",
        "            max_q_next = 0.0\n",
        "\n",
        "        # Q-learning update\n",
        "        Q[state, action] += alpha * (reward + gamma * max_q_next - Q[state, action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # track stats\n",
        "    if reward > 0:\n",
        "        wins += 1\n",
        "    elif reward < 0:\n",
        "        losses += 1\n",
        "    else:\n",
        "        draws += 1\n",
        "\n",
        "    # decay ε\n",
        "    if ep % 5000 == 0:\n",
        "        epsilon = max(0.05, epsilon * 0.9)\n",
        "\n",
        "    # periodic report\n",
        "    if ep % 5000 == 0:\n",
        "        total = wins + draws + losses\n",
        "        print(f\"Episode {ep}: W/D/L = {wins}/{draws}/{losses}  \"\n",
        "              f\"({wins/total:.2f}/{draws/total:.2f}/{losses/total:.2f})\")\n",
        "        wins = draws = losses = 0\n",
        "\n",
        "# ─── 4) Test vs Random Opponent ─────────────────────────────────────────────────\n",
        "test_eps = 0.0\n",
        "test_games = 5000\n",
        "wins = draws = losses = 0\n",
        "\n",
        "for _ in range(test_games):\n",
        "    state = env.reset()\n",
        "    done  = False\n",
        "    while not done:\n",
        "        action = choose_action(Q, state, test_eps)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "    if reward>0: wins +=1\n",
        "    elif reward<0: losses +=1\n",
        "    else: draws +=1\n",
        "\n",
        "print(f\"\\nAfter training: W/D/L = {wins}/{draws}/{losses} \"\n",
        "      f\"({wins/test_games:.2f}/{draws/test_games:.2f}/{losses/test_games:.2f})\")\n"
      ]
    }
  ]
}