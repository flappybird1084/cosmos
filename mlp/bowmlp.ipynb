{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b4d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training...\n",
      "Epoch 1, Loss: 0.3342\n",
      "Epoch 2, Loss: 0.1632\n",
      "Epoch 3, Loss: 0.0991\n",
      "Epoch 4, Loss: 0.0588\n",
      "Epoch 5, Loss: 0.0314\n",
      "Evaluating...\n",
      "Accuracy: 85.60%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Download and extract IMDB data\n",
    "DATA_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "DATA_DIR = 'aclImdb'\n",
    "\n",
    "def download_and_extract():\n",
    "    if not os.path.isdir(DATA_DIR):\n",
    "        filename, _ = urllib.request.urlretrieve(DATA_URL, 'aclImdb_v1.tar.gz')\n",
    "        with tarfile.open(filename, 'r:gz') as tar:\n",
    "            tar.extractall()\n",
    "\n",
    "download_and_extract()\n",
    "\n",
    "# 2. Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def load_imdb_data(split='train'):\n",
    "    data = []\n",
    "    labels = []\n",
    "    split_path = os.path.join(DATA_DIR, split)\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder = os.path.join(split_path, label)\n",
    "        for file in os.listdir(folder):\n",
    "            with open(os.path.join(folder, file), encoding='utf-8') as f:\n",
    "                review = clean_text(f.read())\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_texts, train_labels = load_imdb_data('train')\n",
    "test_texts, test_labels = load_imdb_data('test')\n",
    "\n",
    "# 3. Tokenizer and Vocabulary\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def build_vocab(texts, max_size=20000, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "    vocab = {\"<unk>\": 0}\n",
    "    index = 1\n",
    "    for word, freq in counter.most_common(max_size):\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_texts)\n",
    "\n",
    "# 4. Vectorization using Bag-of-Words\n",
    "def text_to_bow_vector(text, vocab):\n",
    "    vector = torch.zeros(len(vocab), dtype=torch.float32)\n",
    "    for word in tokenize(text):\n",
    "        idx = vocab.get(word, vocab[\"<unk>\"])\n",
    "        vector[idx] += 1\n",
    "    return vector\n",
    "\n",
    "# 5. Create Dataset and DataLoader\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_vector = text_to_bow_vector(self.texts[idx], self.vocab)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return text_vector, label\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = IMDBDataset(train_texts, train_labels, vocab)\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# 6. Define Single Layer Perceptron Model\n",
    "class SLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SLP, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x)).squeeze()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=128):\n",
    "        super(MLP, self).__init__()\n",
    "        if hidden_units > 0:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_units)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(hidden_units, 1)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(input_size, 1)  # Single-layer perceptron (SLP)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if hasattr(self, 'relu'):\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "\n",
    "input_size = len(vocab)\n",
    "# model = SLP(input_size)\n",
    "model = MLP(input_size, hidden_units=128)  # Change hidden_units to 0 for SLP\n",
    "\n",
    "# 7. Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 8. Training Loop\n",
    "def train_model(model, loader, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for features, labels in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 9. Evaluate\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader:\n",
    "            outputs = model(features)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 10. Run training and evaluation\n",
    "print(\"Training...\")\n",
    "train_model(model, train_loader, optimizer, criterion, epochs=5)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff16ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
