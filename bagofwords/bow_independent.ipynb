{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3b9081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Colab cell 1: Install & import\n",
    "!pip install --quiet tensorflow-datasets\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a73b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 18:44:20.128026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752111860.153165  675836 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752111860.161493  675836 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752111860.181614  675836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752111860.181642  675836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752111860.181646  675836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752111860.181651  675836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-09 18:44:20.189207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-09 18:44:24.872513: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-07-09 18:44:25.108037: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-07-09 18:44:35.678631: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-07-09 18:44:46.241100: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Colab cell 2: Load IMDB reviews (raw text) via TFDS\n",
    "ds_train = tfds.load('imdb_reviews', split='train', as_supervised=True)\n",
    "ds_test  = tfds.load('imdb_reviews', split='test',  as_supervised=True)\n",
    "\n",
    "train_texts, train_labels = [], []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(int(label))\n",
    "\n",
    "test_texts, test_labels = [], []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b4d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 25000, Test samples: 25000\n",
      "data head: [\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", 'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_texts)}, Test samples: {len(test_texts)}\")\n",
    "print(f\"data head: {train_texts[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33f73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"Clean text by removing HTML tags, special characters, and extra spaces.\"\"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text into words.\"\"\"\n",
    "    #print(\"cleaning text\")\n",
    "    text = clean(text)\n",
    "    #print(\"tokenizing text\")\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def build_vocabulary(texts, vocab_size=10000):\n",
    "    for text in texts:\n",
    "        tokens = tokenize(text)\n",
    "        counter = Counter(tokens)\n",
    "        vocab = {}\n",
    "        for word, count in counter.most_common(vocab_size):\n",
    "            vocab[word] = count\n",
    "          \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocabulary(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa9223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(text, vocab):\n",
    "  tokens = tokenize(text)\n",
    "  bow = np.zeros(len(vocab), dtype=np.float32)\n",
    "  for token in tokens:\n",
    "    if token in vocab:\n",
    "        index = list(vocab.keys()).index(token)\n",
    "        bow[index] += 1\n",
    "  return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27092d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        bow = text_to_bow(text, self.vocab)\n",
    "        return torch.tensor(bow, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b0fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = IMDBDataset(train_texts, train_labels, vocab)\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af36063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=128):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_units)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(hidden_units, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(input_size=len(vocab), hidden_units=256).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338d8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optimizer, device, epochs=10):\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "      running_loss = 0.0\n",
    "      for inputs, labels in loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          running_loss += loss.item()\n",
    "      print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a061dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 25000, Test samples: 25000\n",
      "Epoch [1/25], Loss: 0.5987\n",
      "Epoch [2/25], Loss: 0.5556\n",
      "Epoch [3/25], Loss: 0.5451\n",
      "Epoch [4/25], Loss: 0.5410\n",
      "Epoch [5/25], Loss: 0.5355\n",
      "Epoch [6/25], Loss: 0.5297\n",
      "Epoch [7/25], Loss: 0.5240\n",
      "Epoch [8/25], Loss: 0.5198\n",
      "Epoch [9/25], Loss: 0.5159\n",
      "Epoch [10/25], Loss: 0.5124\n",
      "Epoch [11/25], Loss: 0.5084\n",
      "Epoch [12/25], Loss: 0.5048\n",
      "Epoch [13/25], Loss: 0.4957\n",
      "Epoch [14/25], Loss: 0.4937\n",
      "Epoch [15/25], Loss: 0.4909\n",
      "Epoch [16/25], Loss: 0.4869\n",
      "Epoch [17/25], Loss: 0.4792\n",
      "Epoch [18/25], Loss: 0.4752\n",
      "Epoch [19/25], Loss: 0.4709\n",
      "Epoch [20/25], Loss: 0.4674\n",
      "Epoch [21/25], Loss: 0.4613\n",
      "Epoch [22/25], Loss: 0.4587\n",
      "Epoch [23/25], Loss: 0.4519\n",
      "Epoch [24/25], Loss: 0.4455\n",
      "Epoch [25/25], Loss: 0.4457\n",
      "Accuracy: 0.7262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73     12500\n",
      "           1       0.72      0.73      0.73     12500\n",
      "\n",
      "    accuracy                           0.73     25000\n",
      "   macro avg       0.73      0.73      0.73     25000\n",
      "weighted avg       0.73      0.73      0.73     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_texts)}, Test samples: {len(test_texts)}\")\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=25)\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "620ebf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: tensor([[ 0.1469, -0.1198]])\n",
      "Sample text sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# test the model with a sample text\n",
    "sample_text = \"This movie was fantastic! I loved it\"\n",
    "sample_text_a = \"good good good good good\"\n",
    "sample_bow = text_to_bow(sample_text, vocab)\n",
    "sample_tensor = torch.tensor(sample_bow, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(sample_tensor)\n",
    "    print(f\"Raw output: {output}\")\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    sentiment = \"Positive\" if predicted.item() == 1 else \"Negative\"\n",
    "    print(f\"Sample text sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573046f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
