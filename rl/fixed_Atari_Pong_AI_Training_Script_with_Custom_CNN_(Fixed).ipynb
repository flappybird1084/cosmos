{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXypDKEdEUFF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Training for ALE/Pong-v5 with Custom CNN ---\n",
            "Logs and models will be saved in: ./data/pong_ppo_custom_cnn_logs/\n",
            "Total timesteps: 250000\n",
            "Number of parallel environments: 4\n",
            "ERROR: Failed to create environment 'ALE/Pong-v5': Namespace ALE not found. Have you installed the proper package for ALE?\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'vec_env' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    111\u001b[39m policy_kwargs = {\n\u001b[32m    112\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeatures_extractor_class\u001b[39m\u001b[33m\"\u001b[39m: CustomCNN,\n\u001b[32m    113\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeatures_extractor_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(features_dim=\u001b[32m256\u001b[39m),\n\u001b[32m    114\u001b[39m }\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# The model is now initialized with the 'CnnPolicy' but its default\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# feature extractor will be replaced by our custom one via `policy_kwargs`.\u001b[39;00m\n\u001b[32m    118\u001b[39m model = PPO(\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCnnPolicy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[43mvec_env\u001b[49m,\n\u001b[32m    121\u001b[39m     policy_kwargs=policy_kwargs,\n\u001b[32m    122\u001b[39m     verbose=\u001b[32m1\u001b[39m,\n\u001b[32m    123\u001b[39m     tensorboard_log=LOG_DIR,\n\u001b[32m    124\u001b[39m     device=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Automatically uses GPU if available, otherwise CPU\u001b[39;00m\n\u001b[32m    125\u001b[39m )\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- PPO model initialized with Custom CNN Policy ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel Architecture:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'vec_env' is not defined"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# @title Atari Pong AI - Training Script with Custom CNN (Saving to Google Drive)\n",
        "\n",
        "# This script trains an AI agent to play Atari Pong using Stable Baselines3 (PPO algorithm)\n",
        "# and Gymnasium. It saves the trained models directly to Google Drive.\n",
        "# This version includes a custom-defined CNN architecture for the policy.\n",
        "\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "# IMPORTANT: Import make_atari_env instead of make_vec_env for proper preprocessing\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# # --- 0. Mount Google Drive ---\n",
        "# print(\"--- Mounting Google Drive ---\")\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "\n",
        "# --- 1. Define Custom CNN Architecture ---\n",
        "# This class defines the neural network that will process the game's image observations.\n",
        "# It inherits from BaseFeaturesExtractor, which is the standard way to create\n",
        "# custom feature extractors in Stable Baselines3.\n",
        "\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    :param observation_space: The observation space of the environment.\n",
        "    :param features_dim: The number of features to extract from the observation.\n",
        "    \"\"\"\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        # We assume CxHxW images (channels first)\n",
        "        # After proper preprocessing, the observation space for Atari is a stack\n",
        "        # of 4 frames of 84x84 pixels (hence, n_input_channels will be 4).\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "\n",
        "        # Define the convolutional layers. You can add or remove layers here.\n",
        "        # This architecture is now receiving the correct (4, 84, 84) input.\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 16, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            # I've restored the third layer as an example of a deeper, valid network.\n",
        "            # You can comment it out again if you wish.\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute the size of the flattened features after the convolutions\n",
        "        # by doing a forward pass with a dummy tensor.\n",
        "        with th.no_grad():\n",
        "            dummy_input = th.as_tensor(observation_space.sample()[None]).float()\n",
        "            n_flatten = self.cnn(dummy_input).shape[1]\n",
        "\n",
        "        # Define the linear layer that comes after the convolutions.\n",
        "        # This layer will output the final feature vector.\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        The forward method defines how input observations are processed by the network.\n",
        "        \"\"\"\n",
        "        # The observations are automatically normalized to [0, 1] by the wrapper.\n",
        "        return self.linear(self.cnn(observations / 255.0))\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "ENV_ID = \"ALE/Pong-v5\"  # The Gymnasium ID for Atari Pong\n",
        "\n",
        "# Set the log directory to a path within your Google Drive\n",
        "LOG_DIR = \"./data/pong_ppo_custom_cnn_logs/\"\n",
        "TOTAL_TIMESTEPS = 250_000  # Total number of timesteps for training\n",
        "SAVE_FREQ = 100_000  # Save model every X timesteps\n",
        "N_ENVS = 4  # Number of parallel environments to run for vectorized training\n",
        "\n",
        "# Create log directory in Google Drive if it doesn't exist\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"--- Starting Training for {ENV_ID} with Custom CNN ---\")\n",
        "print(f\"Logs and models will be saved in: {LOG_DIR}\")\n",
        "print(f\"Total timesteps: {TOTAL_TIMESTEPS}\")\n",
        "print(f\"Number of parallel environments: {N_ENVS}\")\n",
        "\n",
        "# --- Environment Setup ---\n",
        "# *** FIX: Use make_atari_env to apply the correct wrappers ***\n",
        "# This handles frame-stacking, grayscale, resizing, etc., automatically.\n",
        "try:\n",
        "    vec_env = make_atari_env(ENV_ID, n_envs=N_ENVS, seed=0)\n",
        "    # The environment is now automatically wrapped with VecFrameStack and other\n",
        "    # essential preprocessing wrappers for Atari.\n",
        "    print(f\"Successfully created and wrapped vectorized environment for {ENV_ID}\")\n",
        "    print(f\"Corrected Observation space shape: {vec_env.observation_space.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to create environment '{ENV_ID}': {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Model Definition with Custom Policy ---\n",
        "\n",
        "# `policy_kwargs` is a dictionary passed to the model constructor.\n",
        "# It tells the PPO model to use our `CustomCNN` class as the feature extractor.\n",
        "policy_kwargs = {\n",
        "    \"features_extractor_class\": CustomCNN,\n",
        "    \"features_extractor_kwargs\": dict(features_dim=256),\n",
        "}\n",
        "\n",
        "# The model is now initialized with the 'CnnPolicy' but its default\n",
        "# feature extractor will be replaced by our custom one via `policy_kwargs`.\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    vec_env,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=1,\n",
        "    tensorboard_log=LOG_DIR,\n",
        "    device=\"auto\" # Automatically uses GPU if available, otherwise CPU\n",
        ")\n",
        "print(\"\\n--- PPO model initialized with Custom CNN Policy ---\")\n",
        "print(\"Model Architecture:\")\n",
        "print(model.policy)\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# --- Callbacks ---\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=max(SAVE_FREQ // N_ENVS, 1),\n",
        "    save_path=LOG_DIR,\n",
        "    name_prefix=\"pong_ppo_custom_model\"\n",
        ")\n",
        "print(f\"Checkpoint callback set to save every {SAVE_FREQ} total timesteps.\")\n",
        "\n",
        "# --- Training ---\n",
        "print(\"\\n--- Starting Training Process ---\")\n",
        "try:\n",
        "    model.learn(\n",
        "        total_timesteps=TOTAL_TIMESTEPS,\n",
        "        callback=checkpoint_callback,\n",
        "        progress_bar=True\n",
        "    )\n",
        "    print(\"\\nTraining completed!\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred during training: {e}\")\n",
        "\n",
        "# --- Save Final Model ---\n",
        "final_model_path = os.path.join(LOG_DIR, \"pong_ppo_custom_final_model\")\n",
        "model.save(final_model_path)\n",
        "print(f\"Final model saved to: {final_model_path}.zip\")\n",
        "\n",
        "# --- Optional: Evaluation (same as before) ---\n",
        "print(\"\\n--- Evaluation (Optional) ---\")\n",
        "# The evaluation code does not need to be changed.\n",
        "# The loaded model will expect the same preprocessed observations.\n",
        "try:\n",
        "    loaded_model = PPO.load(final_model_path)\n",
        "    eval_env = make_atari_env(ENV_ID, n_envs=1) # Use the same env creation for eval\n",
        "\n",
        "    num_episodes = 5\n",
        "    for episode in range(num_episodes):\n",
        "        obs = eval_env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        print(f\"Starting evaluation episode {episode + 1}/{num_episodes}...\")\n",
        "        while not done:\n",
        "            action, _states = loaded_model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = eval_env.step(action)\n",
        "            episode_reward += reward[0] # Reward is an array in vec env\n",
        "        print(f\"Episode {episode + 1} finished with reward: {episode_reward}\")\n",
        "    eval_env.close()\n",
        "    print(\"Evaluation complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during evaluation: {e}\")\n",
        "    print(\"Evaluation skipped.\")\n",
        "\n",
        "print(\"\\nTo view training progress, you can use TensorBoard:\")\n",
        "print(f\"Load TensorBoard in a new Colab cell with: %load_ext tensorboard\")\n",
        "print(f\"Then run: %tensorboard --logdir {LOG_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
