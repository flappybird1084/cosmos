{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54670334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything ml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import contractions\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "from PIL import ImageDraw\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a683b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to load data from: ./data/decodinglab/placedata.npy\n",
      "File loaded successfully.\n",
      "\n",
      "Successfully extracted all variables. Shapes:\n",
      "  LRtrain:    (359, 916)\n",
      "  LRtest:     (359, 904)\n",
      "  RLtest:     (359, 1003)\n",
      "  LRtraintgt: (916,)\n",
      "  LRtesttgt:  (904,)\n",
      "  RLtesttgt:  (1003,)\n",
      "\n",
      "========================================\n",
      "Analyzing unique labels in target vectors...\n",
      "========================================\n",
      "Unique labels in LRtraintgt: [0.         0.04545455 0.09090909 0.13636364 0.18181818 0.22727273\n",
      " 0.27272727 0.31818182 0.36363636 0.40909091 0.45454545 0.5\n",
      " 0.54545455 0.59090909 0.63636364 0.68181818 0.72727273 0.77272727\n",
      " 0.81818182 0.86363636 0.90909091 0.95454545 1.        ]\n",
      "Unique labels in LRtesttgt:  [0.         0.04545455 0.09090909 0.13636364 0.18181818 0.22727273\n",
      " 0.27272727 0.31818182 0.36363636 0.40909091 0.45454545 0.5\n",
      " 0.54545455 0.59090909 0.63636364 0.68181818 0.72727273 0.77272727\n",
      " 0.81818182 0.86363636 0.90909091 0.95454545 1.        ]\n",
      "Unique labels in RLtesttgt:   [0.         0.04545455 0.09090909 0.13636364 0.18181818 0.22727273\n",
      " 0.27272727 0.31818182 0.36363636 0.40909091 0.45454545 0.5\n",
      " 0.54545455 0.59090909 0.63636364 0.68181818 0.72727273 0.77272727\n",
      " 0.81818182 0.86363636 0.90909091 0.95454545 1.        ]\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "source_path=\"./data/decodinglab/placedata.npy\"\n",
    "# --- 3. Load and Process Data ---\n",
    "print(f\"\\nAttempting to load data from: {source_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the .npy file.\n",
    "    # allow_pickle=True is necessary for loading object arrays, like dictionaries.\n",
    "    # .item() is used to extract the dictionary object that was saved into the file.\n",
    "    loaded_data = np.load(source_path, allow_pickle=True).item()\n",
    "    print(\"File loaded successfully.\")\n",
    "\n",
    "    # --- 4. Extract Variables from the Dictionary ---\n",
    "    # We extract each variable by using its name as a key in the loaded dictionary.\n",
    "    # Note that we are now using 'RLtesttgt' as the key.\n",
    "    LRtrain = loaded_data['LRtrain']\n",
    "    LRtest = loaded_data['LRtest']\n",
    "    RLtest = loaded_data['RLtest']\n",
    "    LRtraintgt = loaded_data['LRtraintgt']\n",
    "    LRtesttgt = loaded_data['LRtesttgt']\n",
    "    RLtesttgt = loaded_data['RLtesttgt'] # <-- Using the new, renamed variable\n",
    "\n",
    "    print(\"\\nSuccessfully extracted all variables. Shapes:\")\n",
    "    print(f\"  LRtrain:    {LRtrain.shape}\")\n",
    "    print(f\"  LRtest:     {LRtest.shape}\")\n",
    "    print(f\"  RLtest:     {RLtest.shape}\")\n",
    "    print(f\"  LRtraintgt: {LRtraintgt.shape}\")\n",
    "    print(f\"  LRtesttgt:  {LRtesttgt.shape}\")\n",
    "    print(f\"  RLtesttgt:  {RLtesttgt.shape}\") # <-- Updated variable name\n",
    "\n",
    "    # --- 5. Find and Print Unique Labels ---\n",
    "    # The np.unique() function returns the sorted unique elements of an array.\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Analyzing unique labels in target vectors...\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    unique_train_labels = np.unique(LRtraintgt)\n",
    "    print(f\"Unique labels in LRtraintgt: {unique_train_labels}\")\n",
    "\n",
    "    unique_test_lr_labels = np.unique(LRtesttgt)\n",
    "    print(f\"Unique labels in LRtesttgt:  {unique_test_lr_labels}\")\n",
    "\n",
    "    unique_test_rl_labels = np.unique(RLtesttgt) # <-- Using the new, renamed variable\n",
    "    print(f\"Unique labels in RLtesttgt:   {unique_test_rl_labels}\") # <-- Updated label\n",
    "    print(\"=\"*40)\n",
    "\n",
    "\n",
    "# --- Error Handling ---\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n[ERROR] File Not Found: The file could not be found at '{source_path}'.\")\n",
    "    print(\"Please ensure that 'placedata_modified.npy' exists in the root of your 'My Drive'.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"\\n[ERROR] Missing Data: The file was loaded, but a required variable key was not found: {e}.\")\n",
    "    print(\"Please check the script that created 'placedata_modified.npy' to ensure all 6 variables were saved correctly with the new names.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fcd3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PlacedataDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, features, targets):\n",
    "#         self.features = features\n",
    "#         self.targets = targets\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         feature = torch.from_numpy(self.features[index]) \n",
    "#         target = torch.tensor(self.targets[index], dtype=torch.float32)\n",
    "#         # print(f\"feature: {feature}\\ntarget: {target}\")\n",
    "#         return feature, target\n",
    "    \n",
    "# # For training data\n",
    "# train_dataset = PlacedataDataset(LRtrain, LRtraintgt)\n",
    "# # print(LRtraintgt)\n",
    "\n",
    "# # For test data (left-to-right)\n",
    "# test_lr_dataset = PlacedataDataset(LRtest, LRtesttgt)\n",
    "\n",
    "# # For test data (right-to-left)\n",
    "# test_rl_dataset = PlacedataDataset(RLtest, RLtesttgt)\n",
    "# train_dataset = PlacedataDataset(LRtrain.T, LRtraintgt)\n",
    "# test_lr_dataset = PlacedataDataset(LRtest.T, LRtesttgt)\n",
    "# test_rl_dataset = PlacedataDataset(RLtest.T, RLtesttgt)\n",
    "\n",
    "\n",
    "# batch_size = 32  # Adjust as needed\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_lr_loader = DataLoader(test_lr_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_rl_loader = DataLoader(test_rl_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1d1d268",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m         optimizer.zero_grad()\n\u001b[32m     67\u001b[39m         loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# --- Evaluation ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/cosmos/env/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/cosmos/env/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/cosmos/env/lib/python3.11/site-packages/torch/optim/adam.py:220\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    214\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[33;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[33;03m            and returns the loss.\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/cosmos/env/lib/python3.11/site-packages/torch/optim/optimizer.py:428\u001b[39m, in \u001b[36mOptimizer._cuda_graph_capture_health_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    413\u001b[39m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[32m    414\u001b[39m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[32m    422\u001b[39m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[32m    423\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    424\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m torch.compiler.is_compiling()\n\u001b[32m    425\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.backends.cuda.is_built()\n\u001b[32m    426\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.cuda.is_available()\n\u001b[32m    427\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m         capturing = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m    431\u001b[39m             group[\u001b[33m\"\u001b[39m\u001b[33mcapturable\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups\n\u001b[32m    432\u001b[39m         ):\n\u001b[32m    433\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    434\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    435\u001b[39m                 + \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m    436\u001b[39m                 + \u001b[33m\"\u001b[39m\u001b[33m but param_groups\u001b[39m\u001b[33m'\u001b[39m\u001b[33m capturable is False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    437\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/.rianstuff/cosmos/env/lib/python3.11/site-packages/torch/cuda/graphs.py:30\u001b[39m, in \u001b[36mis_current_stream_capturing\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_current_stream_capturing\u001b[39m():\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[33;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Parameters ---\n",
    "source_path = \"./data/decodinglab/placedata.npy\"\n",
    "num_inputs = 359\n",
    "num_classes = 23\n",
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def label_to_class(label_array, num_classes=23):\n",
    "    scaled = np.floor(label_array * num_classes).astype(int)\n",
    "    # Ensure maximum label 1.0 maps to class 22 (not 23, which is out of bounds)\n",
    "    scaled[scaled == num_classes] = num_classes - 1\n",
    "    return scaled\n",
    "\n",
    "# --- Load the data ---\n",
    "loaded_data = np.load(source_path, allow_pickle=True).item()\n",
    "LRtrain = loaded_data['LRtrain']\n",
    "LRtest = loaded_data['LRtest']\n",
    "LRtraintgt = loaded_data['LRtraintgt']\n",
    "LRtesttgt = loaded_data['LRtesttgt']\n",
    "\n",
    "# --- Convert targets to class indices ---\n",
    "y_train_cls = label_to_class(LRtraintgt, num_classes)\n",
    "y_test_cls = label_to_class(LRtesttgt, num_classes)\n",
    "\n",
    "# --- Convert to PyTorch tensors ---\n",
    "X_train = torch.tensor(LRtrain.T, dtype=torch.float32)\n",
    "X_test = torch.tensor(LRtest.T, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_cls.T, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_cls.T, dtype=torch.long)\n",
    "\n",
    "# --- Define a Single Layer Perceptron ---\n",
    "class SLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SLP, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SLP(num_inputs, num_classes)\n",
    "\n",
    "# --- Loss and Optimizer ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_train.size(0))\n",
    "\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted_classes = torch.argmax(outputs, dim=1)\n",
    "    accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# --- Optional: Convert Predicted Class Indices back to Center of Position Bin ---\n",
    "def class_to_position(class_indices, num_classes=23):\n",
    "    bin_width = 1.0 / num_classes\n",
    "    return class_indices * bin_width + bin_width / 2  # center of each bin\n",
    "\n",
    "predicted_positions = class_to_position(predicted_classes.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bc401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
